# -*- coding: utf-8 -*-
"""Loan_Predict_Train2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YiTlKX7l8FA55MVohDRRZq-KgxqCTQwS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

df = pd.read_csv('train.csv')
df.head()
df.shape

df.drop(['Loan_ID'],axis =1,inplace=True)

#sns.heatmap(df.isnull(),yticklabels= False, cbar = True)
#sns.countplot(x='Loan_Status',data = df)
#sns.countplot(x='Loan_Status',hue='Property_Area',data = df)

df.info()
df.isnull().sum()

df.Gender.unique()
df.Married.unique()
df.Dependents.unique()
df.Self_Employed.unique()
df.LoanAmount.unique()
df.Loan_Amount_Term.unique()
df.Credit_History.unique()

df.Education.unique()
df.Property_Area.unique()

df['Property_Area'].replace('Semiurban',1,inplace=True) 
df['Property_Area'].replace('Urban',2,inplace=True) 
df['Property_Area'].replace('Rural',3,inplace=True) 

# Handling null values
df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])
df['Married'] = df['Married'].fillna(df['Married'].mode()[0])
df['Dependents'] = df['Dependents'].fillna(df['Dependents'].mode()[0])
df['Self_Employed'] = df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])

df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())
df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mean())

df['Loan_Status'].unique()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Education']=le.fit_transform(df['Education'])
df.head()

le1 = LabelEncoder()
df['Loan_Status']=le1.fit_transform(df['Loan_Status'])
df.head()

df1 = pd.get_dummies(df['Gender'],drop_first=True)
df.drop(['Gender'],axis=1,inplace=True)
df1.head()

df2 = pd.get_dummies(df['Married'],drop_first=True)
df.drop(['Married'],axis=1,inplace=True)
df2.head()

# Dependents feature is ordinal but because of one value it has become categorical. 
#So let's change it...
df['Dependents'].replace('3+',4,inplace=True)

# Convert it into float so that we need not to do any ordinal encoding...
df['Dependents'] = df['Dependents'].astype(float)

df4 = pd.get_dummies(df['Self_Employed'],drop_first=True)
df.drop(['Self_Employed'],axis=1,inplace=True)
df4.head()

df=pd.concat([df1,df2,df4,df],axis=1)

df.head()

X = df.iloc[:,:-1]
y = df.iloc[:,-1]
X.head(5)

y.head()

# Important feature selection using ExtraTreesRegressor

from sklearn.ensemble import ExtraTreesRegressor
selection = ExtraTreesRegressor()
selection.fit(X, y)

print(selection.feature_importances_)

# plot graph of feature importances for better visualization

plt.figure(figsize = (12,8))
feat_importances = pd.Series(selection.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

# eliminate the non important rows

len(df.loc[df['CoapplicantIncome'] == 0])
X.drop(['CoapplicantIncome'], axis=1,inplace = True)
X.drop(['Education'], axis=1,inplace = True)
X.drop(['Male'], axis=1,inplace = True)
X.drop(['Yes'], axis=1,inplace = True)
X.drop(['Property_Area'], axis=1,inplace = True)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Logistic Regression

from sklearn.linear_model import LogisticRegression
log_class = LogisticRegression()
log_class.fit(X_train, y_train)

y_pred = log_class.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
print ('Accuracy:', accuracy_score(y_test, y_pred))
print ('Precision:', precision_score(y_test, y_pred,average='weighted'))                                   
print ('Recall:', recall_score(y_test, y_pred,average='weighted'))
print ('F1 score:', f1_score(y_test, y_pred,average='weighted'))

# Saving model to disk
pickle.dump(log_class, open('model.pkl','wb'))

# Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))
#print(model.predict([[2, 9, 6]]))


X.info()


"""
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
print ('Accuracy:', accuracy_score(y_test, y_pred))
print ('Precision:', precision_score(y_test, y_pred,average='weighted'))                                   
print ('Recall:', recall_score(y_test, y_pred,average='weighted'))
print ('F1 score:', f1_score(y_test, y_pred,average='weighted'))

"""


